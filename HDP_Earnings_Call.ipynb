{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.errors import SettingWithCopyWarning\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "\n",
    "import logging\n",
    "logging.getLogger('yfinance').setLevel(logging.CRITICAL)\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action=\"ignore\", category=[SettingWithCopyWarning, DeprecationWarning])\n",
    "\n",
    "import spacy\n",
    "\n",
    "import pyLDAvis.gensim_models\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "import gensim\n",
    "\n",
    "import tomotopy as tp\n",
    "\n",
    "import yfinance as yf\n",
    "\n",
    "#######################\n",
    "from utils.preprocesing_token import *\n",
    "from utils.hdp_training import *\n",
    "from utils.evaluation import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the ticker and gvkey data\n",
    "comp_info = pd.read_csv(\n",
    "    'data/tick_gvkey_gics.csv'\n",
    "    )\n",
    "\n",
    "# the columns of interest are:\n",
    "# conm: company name\n",
    "# gvkey: unique identifier for the company (Gvkey from S&P Capital IQ Compustat)\n",
    "# tic: ticker symbol\n",
    "# gsector: sector code\n",
    "# gind: industry group code\n",
    "# ggroup: industry code\n",
    "compdesc_info = comp_info[['conm', 'gvkey', 'tic', 'gsector', 'gind', 'ggroup']]\n",
    "# get the unique gvkeys of the data\n",
    "compdesc_info = compdesc_info.drop_duplicates(subset='gvkey')\n",
    "\n",
    "# convert gvkey to string to make it 6 digits\n",
    "compdesc_info = compdesc_info.astype(str)\n",
    "compdesc_info['gvkey'] = compdesc_info['gvkey'].apply(lambda x: x.zfill(6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read transcript data\n",
    "pru_data = pd.read_parquet(\n",
    "    'data/sp500_cc_transcripts_2014_2023.parquet', engine='pyarrow'\n",
    "    )\n",
    "\n",
    "# filter out the pru_data for QnA only\n",
    "# 'transcriptComponentTypeId' = 3 and 4 for the QnA section\n",
    "qna_transcript = pru_data[pru_data['transcriptComponentTypeId'] != 2]\n",
    "# combine all the individual QnA transcripts into one single QnA for each company\n",
    "# for each quarrterly report\n",
    "qna_transcript = qna_transcript.groupby(['gvkey', 'doc_date'])['componentText'].apply(lambda x: ' '.join(x)).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge qna_transcript with compdesc_info on gvkey to get the company name and tickers\n",
    "qna_transcript = qna_transcript.merge(compdesc_info, on='gvkey', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2. Data Preprocessing\n",
    "\n",
    "There are two possible ways of tokenizing the input data for this project:\n",
    "1. Using the PoS except the designated ones \n",
    "    * e.g ['ADV', 'PRON', 'CCONJ', 'PUNCT', 'PART', 'DET', 'ADP', 'SPACE', 'NUM', 'SYM']\n",
    "2. Using only the noun PoS\n",
    "\n",
    "This noteboook will used the second approach, but the first one is also implemented in the codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For the first attempt use the following code to tokenize the text\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "# set noun=True for the second method of tokenization\n",
    "# set noun=False for the first method of tokenization\n",
    "tokens = tokenize_text(qna_transcript, nlp, noun=True)\n",
    "qna_transcript = add_tokenized_text(qna_transcript, tokens)\n",
    "\n",
    "## After the first attempt, save the processed data for future use\n",
    "\n",
    "with open('data/qna_tokens_for_topic.pkl', 'wb') as f:\n",
    "    pickle.dump(tokens, f)\n",
    "\n",
    "qna_transcript.to_parquet('data/qna_transcript_noun_token.parquet', engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Uncomment the following code to use the first method of tokenization\n",
    "\n",
    "# with open('data/qna_noun_tokens_for_topic.pkl', 'wb') as f:\n",
    "#     pickle.dump(tokens, f)\n",
    "# # load tokens from pickle file\n",
    "# with open('data/qna_tokens_for_topic.pkl', 'rb') as f:\n",
    "#     tokens = pickle.load(f)\n",
    "# # save the dataframe to parquet file\n",
    "# qna_transcript.to_parquet('data/qna_transcript_token.parquet', engine='pyarrow')\n",
    "# # code to load pre-tokenized data\n",
    "# qna_transcript = pd.read_parquet(\n",
    "#     'data/qna_transcript_token.parquet', engine='pyarrow'\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For future attempts, simply load the saved data\n",
    "\n",
    "# load tokens from pickle file\n",
    "with open('data/qna_noun_tokens_for_topic.pkl', 'rb') as f:\n",
    "    tokens = pickle.load(f)\n",
    "# code to load pre-tokenized noun data\n",
    "qna_transcript = pd.read_parquet(\n",
    "    'data/qna_transcript_noun_token.parquet', engine='pyarrow'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quarter_lst = qna_transcript['doc_quarter'].unique().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Hierarchical Dirichlet Process (HDP) for Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdp_model_lst = train_hdp_model(qna_transcript, quarter_lst)\n",
    "# save the list of models to file\n",
    "for i, item in enumerate(hdp_model_lst):\n",
    "    # write each item on a new line\n",
    "    item.save(f'hdp_models/hdp_model_{quarter_lst[i]}.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the list of models from file\n",
    "hdp_model_lst = []\n",
    "for quarter in quarter_lst:\n",
    "    mdl = tp.HDPModel.load(f'hdp_models/hdp_model_{quarter}.bin')\n",
    "    hdp_model_lst.append(mdl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earnings_call_qt_list = get_earnings_call_w_topics(hdp_model_lst, qna_transcript)\n",
    "# save earnings_call_qt_list to file\n",
    "with open ('data/earnings_call_qt_list.pkl', 'wb') as f:\n",
    "    pickle.dump(earnings_call_qt_list, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load earnings_call_qt_list from file\n",
    "with open ('data/earnings_call_qt_list.pkl', 'rb') as f:\n",
    "    earnings_call_qt_list = pickle.load(f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HDPEarningsCall",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
