{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.errors import SettingWithCopyWarning\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "\n",
    "import logging\n",
    "logging.getLogger('yfinance').setLevel(logging.CRITICAL)\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action=\"ignore\", category=[SettingWithCopyWarning, DeprecationWarning])\n",
    "\n",
    "import spacy\n",
    "\n",
    "import pyLDAvis.gensim_models\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "import gensim\n",
    "\n",
    "import tomotopy as tp\n",
    "\n",
    "import yfinance as yf\n",
    "\n",
    "#######################\n",
    "from utils.preprocesing_token import *\n",
    "from utils.hdp_training import *\n",
    "from utils.evaluation import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the ticker and gvkey data\n",
    "comp_info = pd.read_csv(\n",
    "    'data/tick_gvkey_gics.csv'\n",
    "    )\n",
    "\n",
    "# the columns of interest are:\n",
    "# conm: company name\n",
    "# gvkey: unique identifier for the company (Gvkey from S&P Capital IQ Compustat)\n",
    "# tic: ticker symbol\n",
    "# gsector: sector code\n",
    "# gind: industry group code\n",
    "# ggroup: industry code\n",
    "compdesc_info = comp_info[['conm', 'gvkey', 'tic', 'gsector', 'gind', 'ggroup']]\n",
    "# get the unique gvkeys of the data\n",
    "compdesc_info = compdesc_info.drop_duplicates(subset='gvkey')\n",
    "\n",
    "# convert gvkey to string to make it 6 digits\n",
    "compdesc_info = compdesc_info.astype(str)\n",
    "compdesc_info['gvkey'] = compdesc_info['gvkey'].apply(lambda x: x.zfill(6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read transcript data\n",
    "pru_data = pd.read_parquet(\n",
    "    'data/sp500_cc_transcripts_2014_2023.parquet', engine='pyarrow'\n",
    "    )\n",
    "\n",
    "# filter out the pru_data for QnA only\n",
    "# 'transcriptComponentTypeId' = 3 and 4 for the QnA section\n",
    "qna_transcript = pru_data[pru_data['transcriptComponentTypeId'] != 2]\n",
    "# combine all the individual QnA transcripts into one single QnA for each company\n",
    "# for each quarrterly report\n",
    "qna_transcript = qna_transcript.groupby(['gvkey', 'doc_date'])['componentText'].apply(lambda x: ' '.join(x)).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge qna_transcript with compdesc_info on gvkey to get the company name and tickers\n",
    "qna_transcript = qna_transcript.merge(compdesc_info, on='gvkey', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2. Data Preprocessing\n",
    "\n",
    "There are two possible ways of tokenizing the input data for this project:\n",
    "1. Using the PoS except the designated ones \n",
    "    * e.g ['ADV', 'PRON', 'CCONJ', 'PUNCT', 'PART', 'DET', 'ADP', 'SPACE', 'NUM', 'SYM']\n",
    "2. Using only the noun PoS\n",
    "\n",
    "This noteboook will used the second approach, but the first one is also implemented in the codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For the first attempt use the following code to tokenize the text\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "# set noun=True for the second method of tokenization\n",
    "# set noun=False for the first method of tokenization\n",
    "tokens = tokenize_text(qna_transcript, nlp, noun=True)\n",
    "qna_transcript = add_tokenized_text(qna_transcript, tokens)\n",
    "\n",
    "## After the first attempt, save the processed data for future use\n",
    "\n",
    "with open('data/qna_tokens_for_topic.pkl', 'wb') as f:\n",
    "    pickle.dump(tokens, f)\n",
    "\n",
    "qna_transcript.to_parquet('data/qna_transcript_noun_token.parquet', engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Uncomment the following code to use the first method of tokenization\n",
    "\n",
    "# with open('data/qna_noun_tokens_for_topic.pkl', 'wb') as f:\n",
    "#     pickle.dump(tokens, f)\n",
    "# # load tokens from pickle file\n",
    "# with open('data/qna_tokens_for_topic.pkl', 'rb') as f:\n",
    "#     tokens = pickle.load(f)\n",
    "# # save the dataframe to parquet file\n",
    "# qna_transcript.to_parquet('data/qna_transcript_token.parquet', engine='pyarrow')\n",
    "# # code to load pre-tokenized data\n",
    "# qna_transcript = pd.read_parquet(\n",
    "#     'data/qna_transcript_token.parquet', engine='pyarrow'\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For future attempts, simply load the saved data\n",
    "\n",
    "# load tokens from pickle file\n",
    "with open('data/qna_noun_tokens_for_topic.pkl', 'rb') as f:\n",
    "    tokens = pickle.load(f)\n",
    "# code to load pre-tokenized noun data\n",
    "qna_transcript = pd.read_parquet(\n",
    "    'data/qna_transcript_noun_token.parquet', engine='pyarrow'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the unique quarters for later use\n",
    "quarter_lst = qna_transcript['doc_quarter'].unique().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Hierarchical Dirichlet Process (HDP) for Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For first attempt, use the following code to train the HDP modle and allocate topics to the transcript\n",
    "\n",
    "# train the HDP model\n",
    "hdp_model_lst = train_hdp_model(qna_transcript, quarter_lst)\n",
    "# allocate topics to the transcript\n",
    "earnings_call_qt_list = get_earnings_call_w_topics(hdp_model_lst, qna_transcript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Uncomment the following code to use the first method of tokenization\n",
    "\n",
    "# # save the list of models to file\n",
    "# for i, item in enumerate(hdp_model_lst):\n",
    "#     item.save(f'hdp_models/hdp_model_{quarter_lst[i]}.bin')\n",
    "# # save earnings_call_qt_list to file\n",
    "# with open ('data/earnings_call_qt_list.pkl', 'wb') as f:\n",
    "#     pickle.dump(earnings_call_qt_list, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For future attempts, simply load the saved models and allocated topics\n",
    "\n",
    "# load the list of models from file\n",
    "hdp_model_lst = []\n",
    "for quarter in quarter_lst:\n",
    "    mdl = tp.HDPModel.load(f'hdp_models/hdp_model_{quarter}.bin')\n",
    "    hdp_model_lst.append(mdl)\n",
    "# load earnings_call_qt_list from file\n",
    "with open ('data/earnings_call_qt_list.pkl', 'rb') as f:\n",
    "    earnings_call_qt_list = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Evaluation of the Results\n",
    "## Step 4-1: Trending Words Change by Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each hdp models, get the top 10 words for each topic\n",
    "top_words = []\n",
    "for i, hdp in enumerate(hdp_model_lst):\n",
    "    top_words.append(get_hdp_topics(hdp, top_n=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_top_wrds_by_qt = []\n",
    "for i in range(len(top_words)):\n",
    "    qt_top_wrds = []\n",
    "    for word_scr_lst in top_words[i].values():\n",
    "        for wrd, scr in word_scr_lst:\n",
    "            qt_top_wrds.append(wrd)\n",
    "    total_top_wrds_by_qt.append(qt_top_wrds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_top_words = []\n",
    "for num_periods in range(len(top_words)):\n",
    "    for key, value in top_words[num_periods].items():\n",
    "        for words in value:\n",
    "            total_top_words.append(words[0])\n",
    "\n",
    "total_top_words = set(total_top_words)\n",
    "total_top_words = list(total_top_words)\n",
    "print(len(total_top_words))\n",
    "\n",
    "# for each list in total_top_wrds_by_qt, get the count of the words in the list that appears in total_top_words\n",
    "# and add as a column to a new dataframe\n",
    "word_count_df = pd.DataFrame()\n",
    "for i in range(len(total_top_wrds_by_qt)):\n",
    "    word_count = []\n",
    "    for word in total_top_words:\n",
    "        word_count.append(total_top_wrds_by_qt[i].count(word))\n",
    "    word_count_df[quarter_lst[i]] = word_count\n",
    "word_count_df.index = total_top_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each row in word_count_df, remove the rows with more than half of the values as 0\n",
    "trimmed_word_count_df = word_count_df.loc[(word_count_df!=0).sum(axis=1) > 10]\n",
    "trimmed_word_count_df.to_csv('data/quarterly_trimmed_word_count.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum the columns of word_count_df based on the year of the column\n",
    "yrly_word_count_df = word_count_df.copy()\n",
    "yrly_word_count_df.columns = word_count_df.columns.to_timestamp().to_period('Y')\n",
    "# groupby columns and sum\n",
    "yrly_word_count_df = yrly_word_count_df.transpose().groupby(level=0).sum().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trim_yrly_word_count_df = yrly_word_count_df.loc[(yrly_word_count_df!=0).sum(axis=1) > 3]\n",
    "trim_yrly_word_count_df.to_csv('data/yearly_trimmed_word_count.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4-2: Performance Evaluation by Grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HDPEarningsCall",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
